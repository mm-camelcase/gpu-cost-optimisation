ollama:
  runtimeClassName: "nvidia"
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1
  resources:
    limits:
      nvidia.com/gpu: 500m  # Allocates half of the GPU to this instance
    requests:
      nvidia.com/gpu: 500m  # Requests half of the GPU

   
  # -- List of models to pull at container startup
  models:
    pull:
      - llama2

extraEnv: 
  - name: NVIDIA_VISIBLE_DEVICES
    value: "all"
  - name: OLLAMA_DEBUG
    value: "1"


# ingress:
#   enabled: true
#   className: "nginx"
#   hosts:
#   - host: ollama.camelcase.club  # Replace with your public IP or domain
#     paths:
#       - path: /mistral
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
#       - path: /llama2
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
