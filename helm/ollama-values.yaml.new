ollama-mistral:
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1  # Both instances share the same GPU
  models:
    pull:
      - mistral

ollama-llama2:
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1  # Same shared GPU
  models:
    pull:
      - llama2

ingress:
  enabled: true
  className: "nginx"
  hosts:
    - host: ollama.camelcase.club
      paths:
        - path: /mistral
          pathType: Prefix
          backend:
            service:
              name: ollama-mistral
              port:
                number: 11434
        - path: /llama2
          pathType: Prefix
          backend:
            service:
              name: ollama-llama2
              port:
                number: 11434

# âœ… Force scheduling on GPU nodes
nodeSelector:
  node-type: gpu  # ðŸ‘ˆ Matches GPU nodegroup labels

# âœ… Tolerate taints applied to GPU nodes
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
