ollama:
  runtimeClassName: "nvidia"
  extraEnv:
    - name: LD_LIBRARY_PATH
      value: "/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu"
  gpu:
    # -- Enable GPU integration
    enabled: true
    
    # -- GPU type: 'nvidia' or 'amd'
    type: 'nvidia'
    
    # -- Specify the number of GPU to 1
    number: 1
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
   
  # -- List of models to pull at container startup
  models:
    pull:
      - mistral
      - llama2

extraEnv: 
  - name: NVIDIA_VISIBLE_DEVICES
    value: "all"
    #value: "GPU-e5ce2210-e846-8263-e9b2-6226ccaa1a24"
  - name: OLLAMA_DEBUG
    value: "1"
  - name: LD_LIBRARY_PATH
    value: "/usr/lib/x86_64-linux-gnu:/lib64"
    #"value": "/lib64:/usr/local/cuda/lib64"
    #value: "/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu"

    #export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/lib64

# ingress:
#   enabled: true
#   className: "nginx"
#   hosts:
#   - host: ollama.camelcase.club  # Replace with your public IP or domain
#     paths:
#       - path: /mistral
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
#       - path: /llama2
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
