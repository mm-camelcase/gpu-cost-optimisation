ollama:
  runtimeClassName: "nvidia"
  gpu:
    # -- Enable GPU integration
    enabled: true
    
    # -- GPU type: 'nvidia' or 'amd'
    type: 'nvidia'
    
    # -- Specify the number of GPU to 1
    number: 1
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
   
  # -- List of models to pull at container startup
  models:
    pull:
      - mistral
      - llama2

# ingress:
#   enabled: true
#   className: "nginx"
#   hosts:
#   - host: ollama.camelcase.club  # Replace with your public IP or domain
#     paths:
#       - path: /mistral
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
#       - path: /llama2
#         pathType: Prefix
#         backend:
#           service:
#             name: ollama
#             port:
#               number: 11434
