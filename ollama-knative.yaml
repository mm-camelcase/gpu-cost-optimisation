apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ollama-gpu
  namespace: default
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/min-scale: "0"   # Scale to zero when idle
        autoscaling.knative.dev/max-scale: "2"   # Allow 2 Ollama instances
        client.knative.dev/revision-timestamp: "2024-02-14T12:00:01Z"  # Force new version
    spec:
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"
      containers:
      - name: ollama-mistral
        image: ollama/ollama:0.5.11
        args: ["mistral"]  # Runs Mistral model
        ports:
          - containerPort: 8080  # ✅ Required for Knative
        resources:
          limits:
            nvidia.com/gpu: "1"  # Uses shared CUDA MPS
      - name: ollama-llama2
        image: ollama/ollama:0.5.11
        args: ["llama2"]  # Runs LLaMA 2 model
        env:
          - name: OLLAMA_MISTRAL_HOST
            value: "localhost:8080"  # ✅ Internal communication to mistral
        resources:
          limits:
            nvidia.com/gpu: "1"  # Uses shared CUDA MPS